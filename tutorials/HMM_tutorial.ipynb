{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blechpy Poisson HMM Tutorial\n",
    "This tutorial will cover how to setup and fit spike data to a poisson HMM.\n",
    "To use this you must first already have a blechpy.dataset object created with data that is past the spike sorting stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "# Imports\n",
    "import blechpy\n",
    "from blechpy.analysis import poissonHMM as phmm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# First get the path to your recording folder\n",
    "rec_dir = '/data/Katz_Data/Stk11_Project/RN10/RN10_ctaTest_190220_131512'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a single HMM\n",
    "To fit a single HMM you will using the PoissonHMM object. This object will house all the necessary parameters for an HMM except for the data being fitted. \n",
    "### Gathering the data\n",
    "First you will need to collect the data for the HMM to fit. This should be a spike array (numpy array, dtype=int32) with 3-dimensions: Trial, Cell, Time bin with each value being the number of spikes in the time bin. If you have sorted your units and then used dat.make_unit_arrays(), then your spike arrays are stored in you h5 file with 1ms time bins from -2000 ms to 5000 ms. You will need to grab this spike array and cut it down to the right time window that you want to model as well as only the units you want to use. Additionally, you may need to rebin this array to have a different time step (especially with sparse firing units). \n",
    "\n",
    "phmm has a useful function for gathering this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "din_channel = 0 # Channel of the digital input that the trials you wish to fit are on\n",
    "unit_type = 'single' # This can be single (for all single units),\n",
    "                     # pyramidal (only single unit regular-spiking cells) \n",
    "                     # or interneuron (only single unit fast-spiking cells)\n",
    "        \n",
    "# The parameters below are optional\n",
    "time_start = 0  # Time start in ms\n",
    "time_end = 2000 # Time end in ms\n",
    "dt = 0.01 # desired bin size for the return spike array in seconds, default is 0.001 seconds\n",
    "\n",
    "spike_array, dt, time = phmm.get_hmm_spike_data(rec_dir, unit_type, din_channel, time_start, time_end, dt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing and fitting the model\n",
    "Now you can go ahead and initialize and fit your HMM.\n",
    "#### Something to note: \n",
    "I have not yet figured out the best parameters to use to test convergence. So for now at each iteration the changes in every matrix (transition, emission and initial distribution) are computed and when the total change in every matrix is below the threshold then fitting stops. Alternatively, fitting stops if the maximum number of iterations is reached. For now the default convergence threshold is 1e-4 which works well for the simulated data. I have not yet had actual data meet this criteria. \n",
    "\n",
    "Also the cost of the model is computed on each iteration. The cost is computed by predicting the state at each time point and then using the emission (rate) matrix to predict the firing rate in each bin and them computing the distance of this prediction from the actual firing rate. This is then summed over time bins and averaged over trials to get the final cost of the model. This would probably provide a better measure of convergence, but I have not yet determined the best threshold for change in cost at which to stop fitting. Also this may lead to overfitting. But cost does provide a means of comparing models since BIC is only a good measure to compare models with the same number of states and time bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "n_states = 3  # Number of predicted states in your data\n",
    "\n",
    "# Initializing the model\n",
    "model = phmm.PoissonHMM(n_states) # Notice you're not giving it the data yet\n",
    "\n",
    "# Fitting the model\n",
    "convergence_threshold = 1e-4  # This is optional, the default is 1e-5, for my final models I used 1e-10.\n",
    "                              # This is the threshold for fitting such that when the change in log_likelihood \n",
    "                              # betweeen iterations is below this then fitting ends.\n",
    "max_iter = 1000  # This is also optional, the default is 1000\n",
    "\n",
    "model.fit(spike_array, dt, time, max_iter=max_iter, thresh=convergence_threshold)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding your model\n",
    "Now that the model is fitted there are some useful aspects to know about it. \n",
    "#### Important Attributes\n",
    "- model.transition\n",
    " - transtion matrix giving probability of switching from one state to another\n",
    "- model.emission\n",
    " - This is actually a rate matrix expressing the predicted firing of each neuron in each state\n",
    "- model.initial_distribution\n",
    " - This gives the probability of being in each state at the start\n",
    "- model.cost\n",
    " - This has the last computed cost of the model\n",
    "- model.BIC\n",
    " - This has the last computed Bayesian Information Criteria of the model\n",
    "- model.best_sequences\n",
    " - This has the best predicted sequence for each trial\n",
    "- model.max_log_prob\n",
    " - Max log probability of the sequences (not sure this is computed correctly)\n",
    " \n",
    "#### Useful fucntions in the model\n",
    "     best_sequences, max_log_prob = model.get_best_paths(spike_array, dt)\n",
    "     forward_probs = model.get_forward_probabilities(spike_array, dt)\n",
    "     bakward_probs = model.get_backward_probabilities(spike_array, dt)\n",
    "     gamma_probs = model.get_gamma_probabilites(spike_array, dt)\n",
    "     \n",
    "Additionally the model keeps a limited history of previous iterations can be be rolled back in case you pass a minima. Use:\n",
    "    `model.set_to_lowest_cost()`\n",
    "    or\n",
    "    `model.set_to_lowest_BIC()`\n",
    "     \n",
    "Finally if you would like to re-fit the model, be sure to randomize it again before refitting:\n",
    "- `model.randomize(spike_array, dt)`\n",
    "- `model.fit(spike_array, dt)`\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and saving HMMs\n",
    "That's just a breakdown of a `PoissonHMM` object. A much better way is to use the `HmmHandler`. This interface handles fitting HMMs for all digital inputs (tastes) as well as trying different parameters sets, plotting and saving all data to an hdf5 store. *This store is seperate from your data h5 file*\n",
    "The HMM handler also takes care of parallelizing HMM fitting and creating plots for fitted HMMs\n",
    "\n",
    "### The Parameters\n",
    "\n",
    "The HmmHandler is passed parameters as a dict or a list of dicts. You can provide as many or as few of these parameters as you want. The defaults are drawn from `phmm.HMM_PARAMS`. These important parameters are:\n",
    "- hmm_id\n",
    " - This is set automatically by the handler\n",
    "- taste\n",
    " - This will be set automatically as all tastes in the dataset, but you can specific a single one if you'd like. **See Updates below**.\n",
    "- channel\n",
    " - This is always set automatically\n",
    "- unit_type\n",
    " - can be 'single', 'pyramidal', or 'interneurons'\n",
    "- dt\n",
    " - time bin size to use in seconds\n",
    "- threshold\n",
    " - the convergence threshold to use for fitting\n",
    "- max_iter\n",
    " - max number of iterations while fitting\n",
    "- n_cells\n",
    " - Set automatically\n",
    "- n_trials\n",
    " - Set automatically\n",
    "- time_start\n",
    " - Time start to cut data\n",
    "- time_end\n",
    " - Time end to cut data\n",
    "- n_repeats\n",
    " - Number of repeats to fit of this HMM, best is chosen automatically by lowest BIC\n",
    "- n_states\n",
    " - Number of predicted states to fit\n",
    "- fitted\n",
    " - set automatically when fitting is compelete\n",
    " \n",
    "Notice that a lot are set automatically, so your input dict can only contain the parameters you want to deviate from the defaults, the rest will be filled in. See the below cell's output to see the default dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "phmm.HMM_PARAMS"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Example of defining parameter set\n",
    "params = [{'unit_type': 'pyramidal', 'time_end': 2500, 'n_states': x} for x in [2,3]]\n",
    "print(params)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can initialize and run the handler\n",
    "Keep in mind after using the handler you can load it again at anytime and add new parameters and re-run, only the model that haven't already been fitted will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initializing\n",
    "handler = phmm.HmmHandler(rec_dir)\n",
    "# Save directory is automatically made inside the recording directory,\n",
    "# but you can also specificy another place witht eh save_dir keyword argument.\n",
    "# You can also pass the params directly when initializing the handler, but\n",
    "# I just split it here you can see how to add new parameters later.\n",
    "handler.add_params(params)\n",
    "\n",
    "# Running the handler\n",
    "handler.run() # to overwrite existing models pass overwrite=True\n",
    "\n",
    "# To plot data\n",
    "handler.plot_saved_models()\n",
    "\n",
    "# Looking at the parameters already in the handler\n",
    "parameter_overview = handler.get_parameter_overview() # this is a pandas DataFrame\n",
    "\n",
    "# Looking at the parameters and fitted model stats\n",
    "data_overview = handler.get_data_overview() # also a pandas DataFrame with extra info such as cost and BIC\n",
    "\n",
    "# The matrices defining each HMM and the best sequences can be access from teh HDF5 store directly. They can also be access programatically with:\n",
    "hdf5_file = handler.h5_file\n",
    "hmm, time, params = handler.get_hmm(0) # the hmm_id number goes here\n",
    "# The hmm object has an attribute stat_arrays with various information including best_sequences, \n",
    "# gamma_probabilities, max_log_prob (on each iteration), time, and row_id\n",
    "# Now you have the PoissonHMM object with the fitted model parameters and can do anything with it. \n",
    "# Only information lost is the model history, every model is set to the best model in the history before saving"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using pre-blechpy spike sorted data, in order to fit HMMs you must first:\n",
    "- create a dataset\n",
    "- initParams\n",
    "- create_trial_list\n",
    "- blechpy.dio.h5io.write_electrode_map_to_h5(dat.h5_file, dat.electrode_mapping)\n",
    "- add array_time vector to the spike_arrays stored in the .h5 file\n",
    "\n",
    "These processes are wrapped into the `blechpy.port_in_dataset` function. \n",
    "```python\n",
    "#Example\n",
    "blechpy.port_in_dataset(rec_dir, shell=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates\n",
    "HmmHandler has some little things about the params you should know.\n",
    "\n",
    "- taste:\n",
    "  - If left as `None`: will fit 1 HMM for each dig_in with spike_array == True in dat.dig_in_mapping\n",
    "  - If `str`: should match the name of a single dig_in in dat.dig_in_mapping, will fit 1 HMM using trials for that taste only\n",
    "  - If `List of str`: Will fit exactly 1 HMM using trials from all digital_input specfied by name in the List. \n",
    "  - If `'all'`: Will fit exactly 1 HMM using trial from all digital_inputs with spike_arrays\n",
    "\n",
    "- n_trials:\n",
    "    - If left as `None`: this will be automatically set as the number of trials used to fit each HMM\n",
    "    - If specfied as an `int`: If the integer N is less then the total number of possible trials (for a particular taste) then the fitting will only be done using the first N trials for each taste.\n",
    "\n",
    "- trial_nums:\n",
    "    - This does not even have to be in the params dictionary at all. You will notice it is not int he default params dict `phmm.HMM_PARAMS`. However if this is provided, it MUST be a `list of int` and will specify the trial numbers to be used in fitting. Importantly, these are the trial numbers for each taste. So if you have 120 trials in a session (30 for each of 4 tastes) and you want the last 10 trials of each tastant to be used in fitting then `params['trial_nums'] = list(range(20,30))` or `params['trials_nums'] = [20,21,22,23,24,25,26,27,28,29]` because python indexes from 0. This will use only those trials to fit the HMM. If its 1 taste per HMM then it will fit the HMM using those 10 trials. If multiple tastes per HMM, then it will use these trials for each taste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
